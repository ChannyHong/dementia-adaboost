#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Adadpted from: chirag212 (https://github.com/chirag126/Automatic_Alzheimer_Detection)
"""

#==============================================================================
#   Lexicosyntatic, semantic and other Feature Extraction 
#==============================================================================

import os
import nltk
import argparse
import numpy as np
import pandas as pd
import math
from scipy import spatial
from nltk.corpus import brown
from collections import Counter
from nltk.stem.wordnet import WordNetLemmatizer 
lmtzr = WordNetLemmatizer()

# POS TAGS
#NN	noun, singular 'desk', #NNS	noun plural	'desks', #NNP	proper noun, singular	'Harrison', #NNPS	proper noun, plural	'Americans'
#PRP	personal pronoun	I, he, she, #PRP$	possessive pronoun	my, his, hers, 
#VB	verb, base form	take, #VBD	verb, past tense	took, #VBG	verb, gerund/present participle	taking
#VBN	verb, past participle	taken, #VBP	verb, sing. present, non-3d	take, #VBZ	verb, 3rd person sing. present	takes
#RB	adverb	very, silently,#RBR	adverb, comparative	better, #RBS	adverb, superlative	best
#CC	coordinating conjunction, #IN	preposition/subordinating conjunction
#TO	to	go 'to' the store, #RP	particle	give up, #MD	modal	could, will
#CD	cardinal digit, #LS	list marker	1), #FW	foreign word, #UH	interjection	errrrrrrrm
#DT	determiner, #PDT	predeterminer	'all the kids'
#EX	existential there (like: "there is" ... think of it like "there exists")
#JJ	adjective	'big', #JJR	adjective, comparative	'bigger', #JJS	adjective, superlative	'biggest'
#POS	possessive ending	parent's
#WDT	wh-determiner	which, #WP	wh-pronoun	who, what
#WP$	possessive wh-pronoun	whose, #WRB	wh-abverb	where, when

def similarity(content, POS_tag):          
    temp_info = nltk.pos_tag(nltk.word_tokenize(content))
    temp_fd = nltk.FreqDist(tag for (word, tag) in temp_info)
    tot_pos = sum([temp_fd[tag] for tag in POS_tag])  #sum(temp_fd.values())
    
    local_pos_vec = []
    for tag in POS_tag:
        if tag in list(temp_fd.keys()):
            local_pos_vec.append(temp_fd[tag]/tot_pos)
        else:
            local_pos_vec.append(0)
            
    return local_pos_vec         

def get_tag_info(data, data_pauses, task_type):    
    
    # ----------------- Initialize ---------------
    data_tag_info = []
    feature_set = []
    ttr = {}
    
    index = range(len(data))  
    columns = [ 'Category', 'ttr', 'R', 'num_concepts_mentioned',  'ARI', 'CLI',
               'prp_count', 'prp_noun_ratio', 'Gerund_count', 'NP_count', 'VP_count', 'word_sentence_ratio', 'MLU',
               'count_pauses', 'count_unintelligible', 'count_trailing', 'count_repetitions', 'SIM_score', 'Bruten'] #,'MMSE']
    
    
    feature_set = pd.DataFrame(index=index, columns=columns)

    # ---------------- Mean MMSE ----------------
    ind_control = np.array(data['category'] == 'Control')
    control_mmse = [ i for i in range(len(ind_control)) if (ind_control[i] == True) &
                    (data.loc[i]['mmse']!=data.loc[i]['mmse'])]
    ind_dementia = np.array([not ind for ind in ind_control])
    dementia_mmse = [ i for i in range(len(ind_dementia)) if (ind_dementia[i] == True) &
                (data.loc[i]['mmse']!=data.loc[i]['mmse'])]
    
    if task_type == 'MMSE':
        data = data.drop(control_mmse, axis=0)
        data = data.drop(dementia_mmse, axis=0)
        
    # --------------- Noun tag and Verb tag lists ---------------   

    noun_list = ['NN', 'NNS', 'NNP', 'NNPS']
    verb_list = ['VB', 'VBD', 'VBG', 'VBN', 'VBP']
    
    # ---------- Define production rules / VP, NP definition ----------
    grammar = r"""
    DTR: {<DT><DT>}
    NP: {<DT>?<JJ>*<NN.*>} 
    PP: {<IN><NP>} 
    VPG: {<VBG><NP | PP>}
    VP: {<V.*><NP | PP>}     
    CLAUSE: {<NP><VP>} 
    """  

    # ----------- Number of concepts mentioned ------------------------

    # SUBJECTS (boy, girl, mother), # PLACES (kitchen, exterior seen through the window), 
    # OBJECTS (cabinet, cookies, counter, curtain, dishes on the counter, faucet, floor, jar, plate, sink, stool, water, window) and 
    # ACTIONS (boy taking the cookie, boy or stool falling, woman drying or washing dishes/plate, 
    # water overflowing or spilling, the girl asking for a cookie, 
    # woman unconcerned by the overflowing, # woman indifferent to the children)

    cookie_pic_list = ['cookie','jar','stool','steal', \
                        'sink','kitchen', \
                        'window','curtain','fall'] 
    list1 = ['mother','woman','lady']
    list2 = ['girl','daughter','sister']
    list3 = ['boy','son','child','kid','brother']
    list4 = ['dish','plate','cup']
    list5 = ['overflow','spill','running']
    list6 = ['dry','wash'] 
    list7 = ['faucet'] 
    list8 = ['counter','cabinet'] 
    list9 = ['water'] 
     
    
    # ---------- Distribution feature ----------    
    text = brown.words(categories='news')
    tag_info = nltk.pos_tag(text)
    tag_fd = nltk.FreqDist(tag for (word, tag) in tag_info)
    del_key = []
    for key in tag_fd.keys():
        if not key.isalpha():
            del_key.append(key)
    while not (del_key == []):
        tag_fd.pop(del_key.pop(), None)

    POS_tag = ['NN', 'IN', 'DT', 'VBD', 'VBFG', 'VBG', 'PRP', 'JJ', 'NNP', 'RB', 'NNS', 'CC']        
    tot_pos = sum([tag_fd[tag] for tag in POS_tag])  #sum(tag_fd.values())
    
    global_pos_vec = []
    for tag in POS_tag:
        if tag in list(tag_fd.keys()):
            global_pos_vec.append(tag_fd[tag]/tot_pos)
        else:
            global_pos_vec.append(0)

    # ---------------------tagging information -------------------
    for i in range(data.shape[0]):
        print (i)
        content = data.loc[i]['data']
        content_pauses = data_pauses.loc[i]['data']
        text = nltk.word_tokenize(content)    
        
        # ========= LEXICOSYNTACTIC FEATURES =========
        
        #  ------- POS tagging ------- 
        tag_info = np.array(nltk.pos_tag(text))
        tag_fd = nltk.FreqDist(tag for i, (word, tag) in enumerate(tag_info))
        freq_tag = tag_fd.most_common()
        data_tag_info.append(freq_tag)
        
        # ------- Lemmatize each word -------    
        #text_root = []
        text_root = [lmtzr.lemmatize(j) for indexj, j in enumerate(text)]
        for indexj, j in enumerate(text):
            if tag_info[indexj,1] in noun_list:
                text_root[indexj] = lmtzr.lemmatize(j) 
            elif tag_info[indexj,1] in verb_list:
                text_root[indexj] = lmtzr.lemmatize(j,'v')             
        
        # ------- Phrase type ------- 
        sentence = nltk.pos_tag(text)
        cp = nltk.RegexpParser(grammar)
        phrase_type = cp.parse(sentence)  
        
        # ------- Pronoun frequency -------
        prp_count = sum([pos[1] for pos in freq_tag if pos[0]=='PRP' or pos[0]=='PRP$'])
        
        # ------- Noun frequency -------
        noun_count = sum([pos[1] for pos in freq_tag if pos[0] in noun_list])
        
        # ------- Gerund frequency -------
        vg_count = sum([pos[1] for pos in freq_tag if pos[0]=='VBG'])
        
        # ------- Pronoun-to-Noun ratio -------
        if noun_count != 0:
            prp_noun_ratio = prp_count/noun_count
        else:
            prp_noun_ratio = prp_count
        
        # Noun phrase, Verb phrase, Verb gerund phrase frequency        
        NP_count = 0
        VP_count = 0
        VGP_count = 0
        for index_t, t in enumerate(phrase_type):
            if not isinstance(phrase_type[index_t],tuple):
                if phrase_type[index_t].label() == 'NP':
                    NP_count = NP_count + 1;
                elif phrase_type[index_t].label() == 'VP': 
                    VP_count = VP_count + 1;
                elif phrase_type[index_t].label() == 'VGP':
                    VGP_count = VGP_count + 1;
                            
        # ------- TTR type-to-token ratio ------- 
        numtokens = len(text)
        freq_token_type = Counter(text)  # or len(set(text)) # text_root
        v = len(freq_token_type)
        ttr = float(v)/numtokens       
                   
        # ------- TTR type-to-token ratio lemmatized------- 
        freq_lemmtoken_type = Counter(text_root)  # or len(set(text)) # text_root
        vl = len(freq_lemmtoken_type)
        ttr_lemmatized = float(vl)/numtokens                         
        
        # ------- Honore's statistic ------- 
        freq_token_root = Counter(text_root)
        occur_once = 0
        for j in freq_token_root:
            if freq_token_root[j] == 1:
                occur_once = occur_once + 1
        v1 = occur_once
        R = 100 * math.log(numtokens / (1 - (v1/v)))
                
        # ------- Automated readability index ------- 
        num_char = len([c for c in content if c.isdigit() or c.isalpha()])
        num_words = len([word for word in content.split(' ') if not word=='' and not word=='.'])
        num_sentences = content.count('.') + content.count('?')
        ARI = 4.71*(num_char/num_words) + 0.5*(num_words/num_sentences) - 21.43
        
        # ------- Colemanâ€“Liau index -------
        L = (num_char/num_words)*100
        S = (num_sentences/num_words)*100
        CLI = 0.0588*L - 0.296*S - 15.8                
            
        # ------- Word-to-sentence_ratio -------
        word_sentence_ratio = num_words/num_sentences
        
        # ------- Mean Length Utterance (MLU) -------
        # Num of utterances based on end of sentence '.' as well as pauses (1pause1., 2pause2., 3pause3. )
        num_utterances = content_pauses.count('.') + content_pauses.count('?')
        MeanLenUtter = num_words/num_utterances
        
        # ========= SEMANTIC FEATURES =========
        
        # ------- Mention of key concepts ------- 
        num_concepts_mentioned = len(set(cookie_pic_list) & set(freq_token_root)) \
                                + len(set(list1) & set(freq_token_root)) + len(set(list2) & set(freq_token_root)) \
                                + len(set(list3) & set(freq_token_root)) + len(set(list4) & set(freq_token_root)) \
                                + len(set(list5) & set(freq_token_root)) + len(set(list6) & set(freq_token_root)) \
                                + len(set(list7) & set(freq_token_root)) + len(set(list8) & set(freq_token_root)) \
                                + len(set(list9) & set(freq_token_root))           								
        
        # ========= ACOUSTIC FEATURES =========
        
        # ------- Pauses and unintelligible count -------
        count_pauses = data.loc[i]['pause2'] + data.loc[i]['pause3']
        
        count_unintelligible = data.loc[i]['count_unintelligible']
        
        count_trailing = data.loc[i]['count_trailing']
        
        count_repetitions = data.loc[i]['count_repetitions']
        
        # ---------- Distribution feature ----------    
        local_pos_vec = similarity(content, POS_tag)
        sim_score = 1 - spatial.distance.cosine(local_pos_vec, global_pos_vec)

        # ---------- Bruten Index ----------    
        bruten = float(vl)**(numtokens**-0.0165)
        
        file_name = data.loc[i]['filepath']
        
        # ----------- Expected output label ---------------
        # -- Dementia classification --
        if data.loc[i]['category'] == 'Control':
            label = 0
        else: 
            label = 1
  
        # -- MMSE prediction --
        if data.loc[i]['mmse'] != data.loc[i]['mmse']:
            if data.loc[i]['category'] == 'Control':
                MMSE = control_mmse
            else:
                MMSE = dementia_mmse
        else:
            MMSE = int(data.loc[i]['mmse'])
                        
        feature_set.loc[i] = [label, ttr, R, num_concepts_mentioned, ARI, CLI, 
                              prp_count, prp_noun_ratio, vg_count, NP_count, VP_count, 
                              word_sentence_ratio, MeanLenUtter, count_pauses, count_unintelligible,
                              count_trailing, count_repetitions, sim_score, bruten]   #,MMSE]
        
 
    return data_tag_info, feature_set         

def main():

    # ------- Input data from metadata.csv ------- 
    parser = argparse.ArgumentParser(description='Processing Dementia data')
    parser.add_argument('--file_path', default=os.path.join(os.getcwd(),'metadata_allvisits.csv'), type=str,
                        help='filepath for Control and Dementia folders')     
    # ------ Data with pauses for MLU computations ------
    parser.add_argument('--file_path2', default=os.path.join(os.getcwd(),'metadata_withPauses.csv'), type=str,
                        help='filepath for Control and Dementia folders')        

    # --------- SELECT Dementia classification (DEM) or MMSE prediction (MMSE) ----- 
    parser.add_argument('--type', default='DEM', type=str,
                help='DEM/MMSE')

    args = parser.parse_args()
    data = pd.read_csv(args.file_path, encoding='utf-8')
    data_pauses = pd.read_csv(args.file_path2, encoding='utf-8')
    
     
    
    # ------- Feature extraction ------- 
    data_tag_info, feature_set = get_tag_info(data, data_pauses, args.type)
    
    # ------- Save to feature_set.csv ------- 
    if args.type == 'MMSE':
        feature_set.to_csv(os.path.join(os.getcwd(), 'feature_set_MMSE.csv'), index=False, encoding="utf8")   
    elif args.type == 'DEM':
        feature_set.to_csv(os.path.join(os.getcwd(), 'feature_set_dem.csv'), index=False, encoding="utf8")   
    else:
        print ('Invalid datatype')
    
    return feature_set
    
if __name__ == '__main__':
    feature_set = main()